#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@authors: Carlos M. Alaíz, Ángela Fernández, Pablo Varona.
"""
import matplotlib
import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
import random
import seaborn as sn
from sklearn.metrics import accuracy_score, log_loss, confusion_matrix
from sklearn.model_selection import train_test_split
from tensorflow import keras

CMAP = "Set1"
FIG_SIZE_1 = 5
FIG_SIZE_2 = 5

"""
EXAMPLE FUNCTIONS
"""


def test_configuration():
    """Tests that the notebook is properly configured."""
    print("¡Bienvenido a los notebooks de ejemplo del curso de Deep Learning!")
    print(
        "Parece que la configuración es correcta, tienes acceso a TensorFlow {}.".format(
            tf.__version__
        )
    )
    print("Recuerda el sentido de la vida (en binario, claro): 101010.")


"""
CONFIGURATION FUNCTIONS
"""


def set_random_seed(seed=123):
    """Initializes the random seed for NumPy, TensorFlow and Keras.

    Args:
        seed (int, optional): Seed number. Defaults to 123.
    """
    np.random.seed(seed)
    tf.random.set_seed(seed)
    tf.keras.utils.set_random_seed(seed)


def configure_plots():
    """Sets the standard configuration for Matplotlib plots."""
    matplotlib.rc("figure", figsize=(FIG_SIZE_1, FIG_SIZE_2))
    matplotlib.rcParams["lines.linewidth"] = 2
    matplotlib.rcParams["axes.prop_cycle"] = matplotlib.cycler(
        color=plt.get_cmap(CMAP)([0, 1, 0.55, 0.33, 0.77, 0.11, 0.22, 0.44, 0.66, 0.88])
    )
    matplotlib.rcParams["legend.loc"] = "best"
    matplotlib.rcParams["legend.fancybox"] = True
    matplotlib.rcParams["legend.shadow"] = True


"""
MODEL FUNCTIONS
"""


def evaluate_perceptron(x, y, w1, w2, b):
    """Computes the predictions of a perceptron over a certain 2-dimensional
    dataset, comparing it with the real labels.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        w1 (float): First weight of the perceptron.
        w2 (float): Second weight of the perceptron.
        b (float): Bias of the perceptron.
    """
    y_s = x @ np.array([w1, w2]) + b
    y_p = np.array(y_s >= 0, dtype=int)

    print("{:^34}\t{:}".format("Predicción", "Real"))
    for x_i, y_i, s_i, p_i in zip(x, y, y_s, y_p):
        print(
            "{:+.1f} x {} {:+.1f} x {} {:+.1f} = {:+.1f} => {}\t{:^4}".format(
                x_i[0], w1, x_i[1], w2, b, s_i, p_i, y_i
            ),
            end="",
        )
        if y_i != p_i:
            print("\t✗")
        else:
            print("\t✓")

    matches = np.sum(y == y_p)
    if matches == len(y):
        print("\nEl perceptrón acierta todas las muestras")
    else:
        print(
            "\nEl perceptrón acierta {} muestras de un total de {}".format(
                matches, len(y)
            )
        )


def evaluate_perceptron_keras(x, y, model, history, title):
    """Shows the separation generated by a Keras perceptron, and the training
    evolution in function of the accuracy.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        model (Keras model): Trained Keras model.
        history (Keras history): Evolution of the training (output of fit).
        title (str): Title of the figure.
    """
    accs = 100 * np.array(history.history["accuracy"])

    vx = np.linspace(np.min(x), np.max(x), 201)
    xx, yy = np.meshgrid(vx, vx)
    xx = xx.ravel()
    yy = yy.ravel()

    plt.figure(figsize=(2 * FIG_SIZE_1, FIG_SIZE_2))

    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(accs) + 1), accs)
    plt.ylim([0, 100])
    plt.xlabel("Época")
    plt.ylabel("Tasa de acierto (%)")
    plt.title("Evolución del entrenamiento")

    plt.subplot(1, 2, 2)
    plot_linear_model_c(
        x,
        y,
        model.layers[0].get_weights()[0].ravel(),
        model.layers[0].get_weights()[1][0],
    )
    plt.title("Modelo final")

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()


def create_perceptron(x, y, epochs=500, seed=123):
    """Creates a perceptron model, which is a Keras model but with a special
    predict function to output directly the predicted class.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        epochs (int, optional): Number of training epochs. Defaults to 500.
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        (Modified) Keras model: Trained Keras model with custom predict method.
    """
    tf.random.set_seed(seed)

    model = keras.Sequential(keras.layers.Dense(1, activation="sigmoid"))
    model.compile(optimizer="adam", loss="binary_crossentropy")
    model.fit(x, y, epochs=epochs, verbose=0)

    pcp = keras.models.clone_model(model)
    pcp.predict = lambda x: np.array(
        [0 if p_i < 0.5 else 1 for p_i in model.predict(x, verbose=0)]
    )

    return pcp


def train_networks(
    model,
    x,
    y,
    epochs=[
        1,
    ],
    seed=123,
):
    """Trains a Keras model, saving and returning a copy of the model in the
    epochs specified.

    Args:
        model (Keras model): Model.
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        epochs (list of int, optional): List of epochs on which to save the model. Defaults to [1,].
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        list of Keras models: Trained models.
    """
    tf.random.set_seed(seed)

    models = []

    for e in epochs:
        model.fit(x, y, epochs=e, verbose=0)

        copy = keras.models.clone_model(model)
        copy.set_weights(model.get_weights())
        models.append(copy)

    random.seed(seed)
    random.shuffle(models)

    return models


def minimize_function(f, t_ini, x, eta, plot=False):
    """Minimizes a TensorFlow function using gradient descent.

    Args:
        f (TensorFlow function): Function.
        t_ini (NumPy array/float): Initial point.
        x (NumPy array): Points for the plot.
        eta (float): Step size.
        plot (bool, optional): Whether to plot or not the evolution. Defaults to False.

    Returns:
        NumPy array/float: Minimizer of the function.
    """
    t = t_ini
    t = tf.Variable(t)

    evo = [t.numpy()]
    for i in range(50):
        with tf.GradientTape() as tape:
            grad = tape.gradient(f(t), t)
            t.assign(t - eta * grad.numpy())
            evo.append(t.numpy())

    evo = np.array(evo)

    if plot:
        plt.plot(x, f(x))
        plt.plot(evo, f(evo), "-*")
        plt.xlabel("$\\theta$")
        plt.ylabel("$\\mathcal{F}(\\theta)$")
        plt.show()

    return t.numpy()


def build_autoencoder(inp_lay, enc_lays, dec_lays, optimizer="adam"):
    """Builds a complete autoencoder, but also its encoder and decoder
    separately.

    Args:
        inp_lay (Keras layer): Input layer corresponding to the data to be encoded.
        enc_lays (Keras layer): Encoder layers which will compress the information.
        dec_lays (Keras layer): Decoder layers which will decompress the information.
        optimizer (str, optional): Optimizer for training the neural network. Defaults to "adam".

    Returns:
        (Keras model): Autoencoder model
        (Keras model): Encoder model
        (Keras model): Decoder model
    """
    autoencoder = keras.Sequential([inp_lay] + enc_lays + dec_lays)
    autoencoder.compile(optimizer=optimizer, loss="mse", metrics=["mse"])

    encoder = keras.Sequential([inp_lay] + enc_lays)

    decoder = keras.Sequential(
        [keras.Input(shape=enc_lays[-1].output_shape[1:])] + dec_lays
    )

    return [autoencoder, encoder, decoder]


def build_generator():
    """Builds a generator that produces an image output of the desired size from
    a random vector (cling to MNIST example).

    Returns:
        (Keras model): Generator.
    """
    generator = keras.Sequential()

    generator.add(keras.Input(shape=(100,)))
    generator.add(keras.layers.Dense(7 * 7 * 128))
    generator.add(keras.layers.Reshape((7, 7, 128)))
    generator.add(
        keras.layers.Conv2DTranspose(128, kernel_size=4, strides=1, padding="same")
    )
    generator.add(keras.layers.LeakyReLU(alpha=0.2))
    generator.add(
        keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding="same")
    )
    generator.add(keras.layers.LeakyReLU(alpha=0.2))
    generator.add(
        keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding="same")
    )
    generator.add(keras.layers.LeakyReLU(alpha=0.2))
    generator.add(
        keras.layers.Conv2D(1, kernel_size=5, padding="same", activation="sigmoid")
    )

    return generator


def build_discriminator():
    """Builds a discriminator with 3 convolutional hidden layers and a
    classification output layer (cling to MNIST example).

    Returns:
        (Keras model): Discriminator.
    """
    discriminator = keras.Sequential()

    discriminator.add(keras.Input(shape=(28, 28, 1)))
    discriminator.add(keras.layers.Conv2D(64, kernel_size=4, strides=2, padding="same"))
    discriminator.add(keras.layers.LeakyReLU(alpha=0.2))
    discriminator.add(
        keras.layers.Conv2D(128, kernel_size=4, strides=2, padding="same")
    )
    discriminator.add(keras.layers.LeakyReLU(alpha=0.2))
    discriminator.add(
        keras.layers.Conv2D(128, kernel_size=4, strides=1, padding="same")
    )
    discriminator.add(keras.layers.LeakyReLU(alpha=0.2))
    discriminator.add(keras.layers.Flatten())
    discriminator.add(keras.layers.Dropout(0.2))
    discriminator.add(keras.layers.Dense(1, activation="sigmoid"))

    discriminator.compile(loss="binary_crossentropy", optimizer="rmsprop")

    return discriminator


def build_gan(discriminator, generator):
    """Builds a GAN. It just consists on the concatenation of a generator and a
    discriminator.

    Args:
        generator (Keras model): Generator model.
        discriminator (Keras model): Discriminator model.

    Returns:
        (Keras model): GAN.
    """
    gan_input = keras.Input(shape=(100,))
    gan = keras.Model(inputs=gan_input, outputs=discriminator(generator(gan_input)))
    gan.compile(loss="binary_crossentropy", optimizer="rmsprop")
    return gan


def train_gan(generator, discriminator, gan, x, max_iter=81, batch_size=128):
    """Trains a GAN.

    Args:
        generator (Keras model): Generator model.
        discriminator (Keras model): Discriminator model.
        gan (Keras model): Complete model.
        x (NumPy array): Input matrix.
        max_iter (int, optional): Maximum number of iterations for convergence. Defaults to 81.
        batch_size (int, optional): Half the size of the minibatch. Defaults to 128.

    Returns:
        (Keras model): Trained GAN.
    """
    input_dim = generator.input_shape[1]

    for i in range(max_iter):
        print("Iteración: %d" % i, end="\r")
        if (i % 10) == 0:
            plot_generated_images(generator)

        noise = np.random.normal(0, 1, [batch_size, input_dim])

        generated_images = generator.predict(noise, verbose=0)
        real_images = x[np.random.randint(low=0, high=x.shape[0], size=batch_size)]

        X = np.concatenate([real_images, generated_images[:, :, :, 0]])
        X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)

        y_dis = np.zeros(2 * batch_size)
        y_dis[:batch_size] = 1.0

        discriminator.trainable = True
        discriminator.train_on_batch(X, y_dis)

        noise = np.random.normal(0, 1, [batch_size, input_dim])
        y_gen = np.ones(batch_size)

        discriminator.trainable = False
        gan.train_on_batch(noise, y_gen)


""""
PLOTTING FUNCTIONS
"""


def plot_dataset_r(x, y, title=None):
    """Depicts a regression dataset.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        title (str, optional): Title of the figure. Defaults to None.
    """
    configure_plots()

    if (len(x.shape) == 1) or (x.shape[1] == 1):
        plt.plot(x.ravel(), y, "*")
        plt.xlabel("$x$")
        plt.ylabel("$y$")

        if title is not None:
            plt.title(title)
        else:
            plt.title("Conjunto de datos")
    else:
        n_plots = x.shape[1]

        plt.figure(figsize=(n_plots * FIG_SIZE_1, FIG_SIZE_2))

        for i in range(n_plots):
            plt.subplot(1, n_plots, i + 1)
            plt.plot(x[:, i], y, "*")
            plt.xlabel("$x_{%d}$" % (i + 1))
            plt.ylabel("$y$")

        if title is not None:
            plt.suptitle(title)
        else:
            plt.suptitle("Conjunto de datos")
        plt.tight_layout()


def plot_temporal_series(x_v, title=None):
    """Depicts a time series dataset.

    Args:
        x_v (list of Numpy arrays): Input time series.
        title (str, optional): Title of the figure. Defaults to None.
    """
    configure_plots()

    ini = 1
    for x in x_v:
        plt.plot(range(ini, ini + len(x.ravel())), x.ravel())
        ini += len(x.ravel())

    plt.xlabel("$t$")
    plt.ylabel("$y$")

    if title is not None:
        plt.title(title)
    else:
        plt.title("Conjunto de datos")


def plot_dataset_c(x, y, title=None):
    """Depicts a classification dataset.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        title (str, optional): Title of the figure. Defaults to None.
    """
    configure_plots()

    if len(x.shape) == 1:
        plt.plot(x, y, "*")
        plt.xlabel("$x$")
        clas = np.unique(y)
        plt.yticks(clas)
    else:
        scatter = plt.scatter(x[:, 0], x[:, 1], c=y, zorder=100, cmap="Set1")
        plt.xlabel("$x_1$")
        plt.ylabel("$x_2$")
        labels = [
            scatter.legend_elements()[0],
            ["Clase ${}$".format(c) for c in np.unique(y)],
        ]
        plt.legend(*labels)

    plt.axis("equal")
    if title is None:
        plt.title("Conjunto de datos")
    else:
        plt.title(title)


def plot_datasets_c(x_v, y_v, title_v=None):
    """Depicts several classification datasets side by side.

    Args:
        x_v (list of Numpy arrays): Input matrices.
        y_v (list of Numpy arrays): Label vectors.
        title_v (list of strings, optional): Titles of the figures. Defaults to None.
    """
    configure_plots()

    n_plots = len(x_v)

    if title_v is None:
        title_v = (None,) * n_plots

    plt.figure(figsize=(n_plots * FIG_SIZE_1, FIG_SIZE_2))
    for i, (x, y, title) in enumerate(zip(x_v, y_v, title_v)):
        plt.subplot(1, n_plots, i + 1)
        plot_dataset_c(x, y, title=title)

    plt.tight_layout()
    plt.show()


def plot_linear_model_c(x, y, w, b):
    """Depicts a linear classification model.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        w (Numpy array): Weight vector.
        b (float): Bias.
    """

    def order_points(points):
        if len(points) == 0:
            return []
        centre = points.mean(axis=0)
        angles = np.arctan2(points[:, 0] - centre[0], points[:, 1] - centre[1])
        o_points = points[np.argsort(angles), :]

        return np.vstack((o_points, o_points[0, :]))

    if (len(x.shape) == 1) or (x.shape[1] != 2):
        raise ValueError("only two-dimensional problems can be represented")

    y_p = np.sign(x @ w + b)

    plot_dataset_c(x, y)
    ax = plt.axis("equal")
    lims = np.array([ax[0] - 100, ax[1] + 100, ax[2] - 100, ax[3] + 100])

    if w[1] != 0:
        x1 = lims[0:2]
        x2 = -(w[0] * x1 + b) / w[1]
    else:
        x2 = lims[2:]
        x1 = -(w[1] * x2 + b) / w[0]

    points = np.column_stack(
        (
            np.append(x1, [lims[0], lims[1], lims[0], lims[1]]),
            np.append(x2, [lims[2], lims[3], lims[3], lims[2]]),
        )
    )

    points_p = order_points(points[points @ w + b >= -1e-2])
    if len(points_p) > 0:
        plt.fill(
            points_p[:, 0], points_p[:, 1], facecolor=plt.get_cmap(CMAP)(1.0), alpha=0.3
        )

    points_n = order_points(points[points @ w + b <= +1e-2])
    if len(points_n) > 0:
        plt.fill(
            points_n[:, 0], points_n[:, 1], facecolor=plt.get_cmap(CMAP)(0.0), alpha=0.3
        )

    plt.plot(x1, x2, "-k")
    plot_dataset_c(x, y)
    plt.axis(ax)

    plt.title(
        "$y = %.2f x_1 + %.2f x_2 + %.2f$ (Acc: %.2f%%)"
        % (w[0], w[1], b, 100 * accuracy_score(y, y_p))
    )


def plot_nonlinear_model_r(x, y, model, title=None, plot_errors=False, show=True):
    """Depicts a non-linear regression model.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        model (Keras/sklearn model): Model.
        title (str, optional): Title of the figure. Defaults to None.
        plot_errors (bool, optional): Whether to plot the errors or not. Defaults to False.
        show (bool, optional): Whether to call pyplot.show(). Defaults to True.
    """
    if isinstance(model, keras.models.Model):
        y_p = model.predict(x, verbose=0)
    else:
        y_p = model.predict(x)

    plt.plot(x, y, "*", label="Obs.")
    plt.plot(x, y_p, "-", label="Pred")

    if plot_errors:
        for i in range(len(x)):
            plt.plot([x[i], x[i]], [y_p[i], y[i]], ":k")

    plt.xlabel("$x$")
    plt.ylabel("$y$")

    if title is not None:
        plt.title(title)

    if show:
        plt.show()


def plot_nonlinear_models_r(x, y, model_v, title_v=None):
    """Depicts several non-linear regression models side by side.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        model_v (list of Keras/sklearn model): Models.
        title_v (str, optional): Titles of the figures. Defaults to None.
    """
    configure_plots()

    n_plots = len(model_v)

    if title_v is None:
        title_v = (None,) * n_plots

    plt.figure(figsize=(n_plots * FIG_SIZE_1, FIG_SIZE_2))
    for i, (model, title) in enumerate(zip(model_v, title_v)):
        plt.subplot(1, n_plots, i + 1)
        plot_nonlinear_model_r(x, y, model, title, show=False)

    plt.tight_layout()
    plt.show()


def plot_nonlinear_model_c(
    x,
    y,
    title=None,
    model=None,
    predict=None,
    x_min=None,
    x_max=None,
    n_points=51,
    show=True,
):
    """Depicts a non-linear classification model.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        title (str, optional): Title of the figure. Defaults to None.
        model (Keras/sklearn model, optional): Model. Defaults to None.
        predict (function, optional): Prediction method. Defaults to None.
        x_min, x_max (float, optional): Axis limits. Defaults to None.
        n_points (int, optional): Number of points for the mesh grid. Defaults to 51.
        show (bool, optional): Whether to call pyplot.show(). Defaults to True.
    """
    if predict is None:
        if isinstance(model, keras.models.Model):
            predict = lambda x: model.predict(x, verbose=0)
        else:
            predict = model.predict

    alpha = 0.3

    configure_plots()

    plot_dataset_c(x, y)
    ax = plt.axis("equal")

    if (x_min is None) or (x_max is None):
        x_1 = np.linspace(plt.xlim()[0], plt.xlim()[1], n_points)
        x_1 = np.hstack((x_1[0] - 100, x_1, x_1[-1] + 100))
        x_2 = np.linspace(plt.ylim()[0], plt.ylim()[1], n_points)
        x_2 = np.hstack((x_2[0] - 100, x_2, x_2[-1] + 100))
    else:
        x_1 = np.linspace(x_min, x_max, n_points)
        x_2 = x_1

    x_1, x_2 = np.meshgrid(x_1, x_2, indexing="ij")

    preds = predict(np.column_stack((x_1.ravel(), x_2.ravel())))
    if not np.issubdtype(preds.dtype, np.integer):
        if preds.shape[1] == 1:
            preds = (preds > 0.5).astype("int")
        else:
            preds = np.argmax(preds, axis=1)
    plt.pcolormesh(
        x_1, x_2, np.reshape(preds, x_1.shape), shading="auto", cmap=CMAP, alpha=alpha
    )

    plot_dataset_c(x, y)
    plt.axis(ax)
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$")

    if title is not None:
        plt.title(title)

    if show:
        plt.show()


def plot_confusion_matrix(x, y, model):
    """Depicts a confusion matrix for evaluating a classification model.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        model (Keras/sklearn model): Model.
    """
    configure_plots()

    plt.figure(figsize=(FIG_SIZE_1, FIG_SIZE_2))

    preds = np.argmax(model.predict(x, verbose=0), axis=1)

    sn.heatmap(
        confusion_matrix(y, preds), annot=True, annot_kws={"fontsize": 6}, cbar=False
    )
    plt.title("Matriz de confusión")
    plt.axis("equal")
    plt.axis("off")

    plt.show()


def plot_image(image, shape=None, title=None, show=True):
    """Depicts an image.

    Args:
        image (array or PIL image): Image.
        shape (ndarray, optional): New shape of the image. Defaults to None.
        title (str, optional): Title of the figure. Defaults to None.
        show (bool, optional): Whether to call pyplot.show(). Defaults to True.
    """
    configure_plots()

    if shape is not None:
        plt.imshow(image.reshape(shape))
    else:
        plt.imshow(image)
    plt.axis("off")

    if title is not None:
        plt.title(title)

    if show:
        plt.show()


def plot_embedding(x, y):
    """Depicts an embedding.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
    """
    configure_plots()

    plt.figure(figsize=(2 * FIG_SIZE_1, FIG_SIZE_2))
    for i in np.unique(y):
        plt.scatter(x[y == i, 0], x[y == i, 1], label="Clase {}".format(i))

    plt.legend()
    plt.show()


def plot_rnn(rnn, x_tr, x_te, y_te, title=None, show=True):
    """Depicts the real data versus the prediction of a RNN model.

    Args:
        rnn (Keras model): RNN model.
        x_tr (NumPy array): Training matrix.
        x_te (NumPy array): Testing matrix.
        y_te (NumPy array): Test label vector.
        title (str, optional): Title of the figure. Defaults to None.
        show (bool, optional): Whether to call pyplot.show(). Defaults to True.
    """
    configure_plots()

    rnn.reset_states()
    rnn.predict(x_tr, batch_size=1, verbose=0)
    preds = rnn.predict(x_te, batch_size=1, verbose=0)

    plt.plot(y_te.ravel(), label="Real")
    plt.plot(preds.ravel(), label="Pred")
    plt.legend()

    plt.xlabel("$t$")
    plt.ylabel("$y$")

    if title is not None:
        plt.title(title)

    if show:
        plt.show()


def plot_samples(x, c="k", s=50, marker="*", show=True):
    """Marks some samples in a plot.

    Args:
        x (NumPy array): Samples.
        c (str, optional): Colour. Defaults to "k".
        s (int, optional): Size. Defaults to 50.
        marker (str, optional): Marker. Defaults to "*".
        show (bool, optional): Whether to call pyplot.show(). Defaults to True.
    """
    plt.scatter(x[:, 0], x[:, 1], c=c, marker=marker, s=s, zorder=1e5)

    if show:
        plt.show()


def plot_training(history, yscale="linear", mark_best=False):
    """Plots the training evolution of a Keras model.

    Args:
        history (History): History returned by fit.
        yscale (str, optional): Scale for y-axis. Defaults to "linear".
        mark_best (bool, optional): Whether to mark, or not, the best result. Defaults to False.
    """
    configure_plots()

    if "accuracy" in history.history:
        plt.figure(figsize=(2 * FIG_SIZE_1, FIG_SIZE_2))
        plt.subplot(1, 2, 1)

    plt.plot(history.history["loss"], label="Entrenamiento")
    if mark_best:
        ind = np.argmin(history.history["loss"])
        plt.plot(ind, history.history["loss"][ind], "*k")

    if "val_loss" in history.history:
        plt.plot(history.history["val_loss"], label="Validación")
        if mark_best:
            ind = np.argmin(history.history["val_loss"])
            plt.plot(ind, history.history["val_loss"][ind], "*k")
        plt.legend()

    plt.xlabel("Época")
    plt.ylabel("Función de coste")
    plt.yscale(yscale)

    if "accuracy" in history.history:
        plt.subplot(1, 2, 2)

        plt.plot(history.history["accuracy"], label="Entrenamiento")
        if "val_accuracy" in history.history:
            plt.plot(history.history["val_accuracy"], label="Validación")
            plt.legend()
        plt.xlabel("Época")
        plt.ylabel("Tasa de acierto (%)")
        plt.suptitle("Evolución del entrenamiento")
        plt.tight_layout()
    else:
        plt.title("Evolución del entrenamiento")


def plot_trainings(history_v, label_v=None, yscale="linear"):
    """Plots the training evolution of several Keras models.

    Args:
        history_v (list of History): Histories returned by fit.
        label_v (list of str, optional): Labels. Defaults to None.
        yscale (str, list of str): Scale for the y-axis. Defaults to "linear".
    """
    configure_plots()

    n_plots = len(history_v)

    if label_v is None:
        label_v = (None,) * n_plots

    for history, label in zip(history_v, label_v):
        plt.plot(history.history["loss"], label=label)

    plt.xlabel("Época")
    plt.ylabel("Función de coste")
    plt.title("Evolución del entrenamiento")
    plt.legend()
    plt.yscale(yscale)


def plot_generated_images(generator, dim=(5, 5)):
    """Depicts the GAN generated images.

    Args:
        generator (Keras model): Generator model.
        dim (ndarray , optional): Subplot structure of the samples. Default to (5,5).
    """
    configure_plots()

    examples = np.prod(dim)
    noise = np.random.normal(loc=0, scale=1, size=[examples, generator.input_shape[1]])

    generated_images = generator.predict(noise, verbose=0)
    generated_images = generated_images.reshape(examples, 28, 28)

    plt.figure(figsize=(dim[0] * FIG_SIZE_1 / 4, dim[1] * FIG_SIZE_2 / 4))
    for i in range(examples):
        plt.subplot(dim[0], dim[1], i + 1)
        plot_image(generated_images[i], show=False)

    plt.tight_layout()
    plt.show()


def print_model_c(x, model):
    """Prints the prediction of a classification model over certain patterns.

    Args:
        x (NumPy array): Input matrix.
        model (Keras/sklearn model): Model.
    """
    preds = model.predict(x, verbose=0)
    n_clas = preds.shape[1]

    if n_clas == 1:
        preds = np.column_stack((1 - preds, preds))
        n_clas = n_clas + 1

    print("{:^25s}".format("Patrón"), end="")
    for i in range(n_clas):
        print("{:^15s}".format("Clase {}".format(i)), end="")
    print("")

    for point, pred in zip(x, preds):
        print("{:^25s}".format("({:g},{:g})".format(point[0], point[1])), end="")
        for p in pred:
            print("{:^15s}".format("{:4.1f}%".format(100 * p)), end="")
        print("")


def print_samples(x_1, x_2=None, samples=None):
    """Prints some samples.

    Args:
        x_1 (NumPy array): Input matrix (original).
        x_2 (NumPy array, optional): Input matrix (modified). Defaults to None.
        samples (list of integers, optional): Indices of the samples to print. Defaults to None.
    """

    def print_sample(x):
        print("[", end="")
        for n in x:
            print("{:6.3f}".format(n), end=" ")
        print("]", end="")

    if samples is None:
        samples = [0, int(len(x_1) / 2), -1]

    print("Algunos patrones de ejemplo:")
    for i in samples:
        print_sample(x_1[i])
        if x_2 is not None:
            print(" => ", end="")
            print_sample(x_2[i])
        print("")


def compare_images(images_1, images_2, shape):
    """Depicts several images for comparison purposes.

    Args:
        image_1 (array or PIL image): Image.
        image_2 (array or PIL image): Image.
        shape (ndarray, optional): New shape of the image. Defaults to None.
    """
    configure_plots()

    n_plots = len(images_1)

    plt.figure(figsize=(n_plots * FIG_SIZE_1 / 2, FIG_SIZE_2))
    for i in range(n_plots):
        plt.subplot(2, n_plots, i + 1)
        plot_image(images_1[i], shape=shape, show=False)

        plt.subplot(2, n_plots, i + 1 + n_plots)
        plot_image(images_2[i], shape=shape, show=False)

    plt.show()


def compare_models_c(x, y, models):
    """Depicts several classification models, comparing their crossentropy.

    Args:
        x (NumPy array): Input matrix.
        y (Numpy array): Label vector.
        models (list of Keras models): Models.
    """
    plt.figure(figsize=(len(models) * FIG_SIZE_1, FIG_SIZE_2))

    for cnt, model in enumerate(models):
        plt.subplot(1, len(models), cnt + 1)
        plot_nonlinear_model_c(x, y, model=model, show=False)
        plt.title(
            "Modelo {} (entropía cruzada: {:.3f})".format(
                cnt + 1, log_loss(y, model.predict(x, verbose=0).ravel())
            )
        )

    plt.tight_layout()
    plt.show()


"""
FUNCTIONS FOR BUILDING DATASETS
"""


def make_and(n_reps=1, noise=0, seed=123):
    """Generates the AND dataset.

    Args:
        n_reps (int, optional): Number of repetition of each sample. Defaults to 1.
        noise (int, optional): Noise level. Defaults to 0.
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    np.random.seed(seed)

    x = np.repeat(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), n_reps, axis=0)
    x = x + noise * np.random.randn(*x.shape)

    y = np.repeat(np.array([0, 0, 0, 1]), n_reps)

    return x, y


def make_or(n_reps=1, noise=0, seed=123):
    """Generates the OR dataset.

    Args:
        n_reps (int, optional): Number of repetition of each sample. Defaults to 1.
        noise (int, optional): Noise level. Defaults to 0.
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    np.random.seed(seed)

    x = np.repeat(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), n_reps, axis=0)
    x = x + noise * np.random.randn(*x.shape)

    y = np.repeat(np.array([0, 1, 1, 1]), n_reps)

    return x, y


def make_xor(n_reps=1, noise=0, seed=123):
    """Generates the XOR dataset.

    Args:
        n_reps (int, optional): Number of repetition of each sample. Defaults to 1.
        noise (int, optional): Noise level. Defaults to 0.
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    np.random.seed(seed)

    x = np.repeat(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), n_reps, axis=0)
    x = x + noise * np.random.randn(*x.shape)

    y = np.repeat(np.array([0, 1, 1, 0]), n_reps)

    return x, y


def make_regression_1(n_pat=1000, noise=1e-1, seed=123):
    """Generates a non-linear regression dataset.

    Args:
        n_pat (int, optional): Number of samples. Defaults to 1000.
        noise (float, optional): Noise level. Defaults to 1e-1.
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    np.random.seed(seed)

    x = np.linspace(-3, 3, n_pat)
    y = x**3 + (x - 1) ** 2 - (x + 1) ** 2
    y = y + noise * np.random.randn(len(y))

    x = x.reshape(-1, 1)

    return x, y


def make_regression_2(n_pat=601, plot=False):
    """Generates a non-linear regression dataset.

    Args:
        n_pat (int, optional): Number of samples. Defaults to 601.
        plot (bool, optional): Whether to plot or not the dataset. Defaults to False.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    x = np.linspace(-3, 3, n_pat)
    y = (x - 1) ** 2 - 2 * np.cos(3 * x - 3)

    if plot:
        plot_dataset_r(
            x, y, title=r"$\mathcal{F}(\theta) = (\theta - 1)^2 - 2 \cos(3 \theta - 3)$"
        )
        plt.xlabel("$\\theta$")
        plt.ylabel("$\\mathcal{F}(\\theta)$")

    return x, y


def make_regression_3(n_pat=100, seed=123):
    """Generates a non-linear regression dataset with a categorical variable.

    Args:
        n_pat (int, optional): Number of samples. Defaults to 100.
        seed (int, optional): Random seed. Defaults to 123.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    set_random_seed(seed)
    x = np.column_stack((np.repeat([1, 2, 3], n_pat), np.random.randn(3 * n_pat)))
    y = np.power(x[:, 1], x[:, 0])

    return x, y


def make_regression_4(n_pat=500, seed=123, noise=5e-1):
    """Generates a non-linear regression dataset.

    Args:
        n_pat (int, optional): Number of samples. Defaults to 500.
        seed (int, optional): Random seed. Defaults to 123.
        noise (float, optional): Noise level. Defaults to 1e-1.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
    """
    set_random_seed(seed)

    x = np.random.randn(n_pat, 2)
    y = np.sum(np.power(x, 2), axis=1)
    y = y + noise * np.random.randn(n_pat)

    return x, y


def make_mnist(flat=False, digit=None):
    """Loads the MNIST dataset.

    Args:
        flat (bool, optional): Whether to flatt or not the data. Defaults to False.
        digit (int, optional): Return the patterns for a particular digit; if None is selected all the digits are part of the dataset returned. Defaults to None.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
        (NumPy array, NumPy array): Output matrix, label vector.

    """
    (x_tr, y_tr), (x_te, y_te) = keras.datasets.mnist.load_data()

    x_tr = x_tr.astype("float32") / 255.0
    x_te = x_te.astype("float32") / 255.0

    if flat:
        x_tr = x_tr.reshape(len(x_tr), -1)
        x_te = x_te.reshape(len(x_te), -1)
    else:
        x_tr = x_tr.reshape(-1, 28, 28, 1)
        x_te = x_te.reshape(-1, 28, 28, 1)

    if digit is not None:
        return x_tr[y_tr == digit, :, :, 0], x_te[y_te == digit, :, :, 0]
    else:
        return (x_tr, y_tr), (x_te, y_te)


def make_temporal_series(n_pat=513):
    """Generates a time series.

    Args:
        n_pat (int, optional): Number of samples. Defaults to 513.

    Returns:
        (NumPy array, NumPy array): Input matrix, label vector.
        (NumPy array, NumPy array): Output matrix, label vector.
    """
    x = np.linspace(-8 * np.pi, 8 * np.pi, n_pat)
    x = np.sin(x)

    y = x[1:].reshape(-1, 1)
    x = x[:-1].reshape(-1, 1, 1)

    x_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size=0.3, shuffle=False)

    return (x_tr, y_tr), (x_te, y_te)
